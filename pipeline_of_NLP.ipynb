{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Mayank\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is my first Natural Langauage Processing task, and i am learning it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'is', 'my'),\n",
       " ('is', 'my', 'first'),\n",
       " ('my', 'first', 'Natural'),\n",
       " ('first', 'Natural', 'Langauage'),\n",
       " ('Natural', 'Langauage', 'Processing'),\n",
       " ('Langauage', 'Processing', 'task'),\n",
       " ('Processing', 'task', ','),\n",
       " ('task', ',', 'and'),\n",
       " (',', 'and', 'i'),\n",
       " ('and', 'i', 'am'),\n",
       " ('i', 'am', 'learning'),\n",
       " ('am', 'learning', 'it'),\n",
       " ('learning', 'it', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(ml_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mayank Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Mayank Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('This', 'DT')],\n",
       " [('is', 'VBZ')],\n",
       " [('my', 'PRP$')],\n",
       " [('first', 'RB')],\n",
       " [('Natural', 'JJ')],\n",
       " [('Langauage', 'NN')],\n",
       " [('Processing', 'VBG')],\n",
       " [('task', 'NN')],\n",
       " [(',', ',')],\n",
       " [('and', 'CC')],\n",
       " [('i', 'NN')],\n",
       " [('am', 'VBP')],\n",
       " [('learning', 'VBG')],\n",
       " [('it', 'PRP')],\n",
       " [('.', '.')]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dont forget [] for tokens\n",
    "[nltk.pos_tag([token]) for token in ml_tokens]\n",
    "\n",
    "# This cannot differentiate b/w a noun or token based on context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mayank\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'first',\n",
       " 'Natural',\n",
       " 'Langauage',\n",
       " 'Processing',\n",
       " 'task',\n",
       " ',',\n",
       " 'learning',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = [word for word in ml_tokens if word not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test --> test\n",
      "testing --> test\n",
      "taste --> tast\n",
      "tester --> tester\n",
      "tastier --> tastier\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemer = PorterStemmer()\n",
    "\n",
    "words = ['test','testing','taste','tester','tastier']\n",
    "\n",
    "for i in words:\n",
    "    print(f'{i} --> {stemer.stem(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test--> test\n",
      "testing--> test\n",
      "taste--> tast\n",
      "tester--> tester\n",
      "tastier--> tastier\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "Snowball = SnowballStemmer(language='english')\n",
    "\n",
    "for i in words:\n",
    "    print(f'{i}--> {Snowball.stem(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-->test\n",
      "testing-->test\n",
      "taste-->tast\n",
      "tester-->test\n",
      "tastier-->tasty\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lanc = LancasterStemmer()\n",
    "\n",
    "for i in words:\n",
    "    print(f'{i}-->{lanc.stem(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test--> test\n",
      "testing--> test\n",
      "taste--> tast\n",
      "tester--> test\n",
      "tastier--> tast\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg = RegexpStemmer(\"ing|e$|er$|ier$\")\n",
    "\n",
    "for i in words:\n",
    "    print(f'{i}--> {reg.stem(i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mayank\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-->test\n",
      "testing-->testing\n",
      "taste-->taste\n",
      "tester-->tester\n",
      "tastier-->tastier\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "for i in words:\n",
    "    print(f'{i}-->{lemma.lemmatize(i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('throng.v.01') press tightly together or cram\n",
      "Synset('jam.v.05') get stuck and immobilized\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "meaning = lesk(word_tokenize(\"I love peanut butter jam.\"),'jam')\n",
    "print(meaning,meaning.definition())\n",
    "\n",
    "take2 = lesk(word_tokenize('I hate getting stuck in traffic jam.'),'jam')\n",
    "print(take2,take2.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\Mayank\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Mayank Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Mayank\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE   (GPE Apple/NNP)\n",
      "GSP   (GSP US/NN)\n"
     ]
    }
   ],
   "source": [
    "text = \"Apple is a big company based in US\"\n",
    "for w in nltk.word_tokenize(text):\n",
    "    for chunck in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(w))):\n",
    "        if hasattr(chunck,'label'):\n",
    "            print(chunck.label(),\" \",chunck)\n",
    "\n",
    "\n",
    "# GPE : Geopolitical Entitiy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") #en_core_web_md (medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "This is one of the greatest films ever made. Brilliant acting by George C. Scott and Diane Riggs. \n",
    "This movie is both disturbing and extremely deep. Don't be fooled into believing this is just a comedy. \n",
    "It is a brilliant satire about the medical profession. It is not a pretty picture.\n",
    " Healthy patients are killed by incompetent surgeons, who spend all their time making money outside the hospital. \n",
    " And yet, you really believe that this is a hospital. \n",
    " The producers were very careful to include real medical terminology and real medical cases. \n",
    " This movie really reveals how difficult in is to run a hospital, and how badly things already were in 1971. \n",
    " I loved this movie. P.S. - I noticed that the incompetent, wheeler dealer surgeon played the head of the firm in \n",
    " LA Law. The young doctor played in Lou Grant. \n",
    " I also noticed that the registration nurse has appeared since in Becker and other shows.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text) # -> automatically does tagger, parser and NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT            | LEMMA_          | POS_     | TAG_     | DEP_        | SHAPE_   | IS_ALPHA | IS_STOP  | \n",
      "\n",
      "               | \n",
      "               | SPACE    | _SP      | dep         | \n",
      "        |        0 |        0 |\n",
      "This            | this            | PRON     | DT       | nsubj       | Xxxx     |        1 |        1 |\n",
      "is              | be              | AUX      | VBZ      | ROOT        | xx       |        1 |        1 |\n",
      "one             | one             | NUM      | CD       | attr        | xxx      |        1 |        1 |\n",
      "of              | of              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
      "the             | the             | DET      | DT       | det         | xxx      |        1 |        1 |\n",
      "greatest        | great           | ADJ      | JJS      | amod        | xxxx     |        1 |        0 |\n",
      "films           | film            | NOUN     | NNS      | pobj        | xxxx     |        1 |        0 |\n",
      "ever            | ever            | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
      "made            | make            | VERB     | VBN      | acl         | xxxx     |        1 |        1 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "Brilliant       | brilliant       | ADJ      | JJ       | ROOT        | Xxxxx    |        1 |        0 |\n",
      "acting          | act             | VERB     | VBG      | acl         | xxxx     |        1 |        0 |\n",
      "by              | by              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
      "George          | George          | PROPN    | NNP      | compound    | Xxxxx    |        1 |        0 |\n",
      "C.              | C.              | PROPN    | NNP      | compound    | X.       |        0 |        0 |\n",
      "Scott           | Scott           | PROPN    | NNP      | pobj        | Xxxxx    |        1 |        0 |\n",
      "and             | and             | CCONJ    | CC       | cc          | xxx      |        1 |        1 |\n",
      "Diane           | Diane           | PROPN    | NNP      | compound    | Xxxxx    |        1 |        0 |\n",
      "Riggs           | Riggs           | PROPN    | NNP      | conj        | Xxxxx    |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "\n",
      "               | \n",
      "               | SPACE    | _SP      | dep         | \n",
      "        |        0 |        0 |\n",
      "This            | this            | DET      | DT       | det         | Xxxx     |        1 |        1 |\n",
      "movie           | movie           | NOUN     | NN       | nsubj       | xxxx     |        1 |        0 |\n",
      "is              | be              | AUX      | VBZ      | ROOT        | xx       |        1 |        1 |\n",
      "both            | both            | PRON     | DT       | preconj     | xxxx     |        1 |        1 |\n",
      "disturbing      | disturbing      | ADJ      | JJ       | acomp       | xxxx     |        1 |        0 |\n",
      "and             | and             | CCONJ    | CC       | cc          | xxx      |        1 |        1 |\n",
      "extremely       | extremely       | ADV      | RB       | advmod      | xxxx     |        1 |        0 |\n",
      "deep            | deep            | ADJ      | JJ       | conj        | xxxx     |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "Do              | do              | AUX      | VB       | aux         | Xx       |        1 |        1 |\n",
      "n't             | not             | PART     | RB       | neg         | x'x      |        0 |        1 |\n",
      "be              | be              | AUX      | VB       | auxpass     | xx       |        1 |        1 |\n",
      "fooled          | fool            | VERB     | VBN      | ROOT        | xxxx     |        1 |        0 |\n",
      "into            | into            | ADP      | IN       | prep        | xxxx     |        1 |        1 |\n",
      "believing       | believe         | VERB     | VBG      | pcomp       | xxxx     |        1 |        0 |\n",
      "this            | this            | PRON     | DT       | nsubj       | xxxx     |        1 |        1 |\n",
      "is              | be              | AUX      | VBZ      | advcl       | xx       |        1 |        1 |\n",
      "just            | just            | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
      "a               | a               | DET      | DT       | det         | x        |        1 |        1 |\n",
      "comedy          | comedy          | NOUN     | NN       | attr        | xxxx     |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "\n",
      "               | \n",
      "               | SPACE    | _SP      | dep         | \n",
      "        |        0 |        0 |\n",
      "It              | it              | PRON     | PRP      | nsubj       | Xx       |        1 |        1 |\n",
      "is              | be              | AUX      | VBZ      | ROOT        | xx       |        1 |        1 |\n",
      "a               | a               | DET      | DT       | det         | x        |        1 |        1 |\n",
      "brilliant       | brilliant       | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
      "satire          | satire          | NOUN     | NN       | attr        | xxxx     |        1 |        0 |\n",
      "about           | about           | ADP      | IN       | prep        | xxxx     |        1 |        1 |\n",
      "the             | the             | DET      | DT       | det         | xxx      |        1 |        1 |\n",
      "medical         | medical         | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
      "profession      | profession      | NOUN     | NN       | pobj        | xxxx     |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "It              | it              | PRON     | PRP      | nsubj       | Xx       |        1 |        1 |\n",
      "is              | be              | AUX      | VBZ      | ROOT        | xx       |        1 |        1 |\n",
      "not             | not             | PART     | RB       | neg         | xxx      |        1 |        1 |\n",
      "a               | a               | DET      | DT       | det         | x        |        1 |        1 |\n",
      "pretty          | pretty          | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
      "picture         | picture         | NOUN     | NN       | attr        | xxxx     |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "\n",
      "               | \n",
      "               | SPACE    | _SP      | dep         | \n",
      "        |        0 |        0 |\n",
      "Healthy         | healthy         | ADJ      | JJ       | amod        | Xxxxx    |        1 |        0 |\n",
      "patients        | patient         | NOUN     | NNS      | nsubjpass   | xxxx     |        1 |        0 |\n",
      "are             | be              | AUX      | VBP      | auxpass     | xxx      |        1 |        1 |\n",
      "killed          | kill            | VERB     | VBN      | ROOT        | xxxx     |        1 |        0 |\n",
      "by              | by              | ADP      | IN       | agent       | xx       |        1 |        1 |\n",
      "incompetent     | incompetent     | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
      "surgeons        | surgeon         | NOUN     | NNS      | pobj        | xxxx     |        1 |        0 |\n",
      ",               | ,               | PUNCT    | ,        | punct       | ,        |        0 |        0 |\n",
      "who             | who             | PRON     | WP       | nsubj       | xxx      |        1 |        1 |\n",
      "spend           | spend           | VERB     | VBP      | relcl       | xxxx     |        1 |        0 |\n",
      "all             | all             | PRON     | DT       | predet      | xxx      |        1 |        1 |\n",
      "their           | their           | PRON     | PRP$     | poss        | xxxx     |        1 |        1 |\n",
      "time            | time            | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
      "making          | make            | VERB     | VBG      | xcomp       | xxxx     |        1 |        0 |\n",
      "money           | money           | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
      "outside         | outside         | ADP      | IN       | prep        | xxxx     |        1 |        0 |\n",
      "the             | the             | DET      | DT       | det         | xxx      |        1 |        1 |\n",
      "hospital        | hospital        | NOUN     | NN       | pobj        | xxxx     |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "\n",
      "               | \n",
      "               | SPACE    | _SP      | dep         | \n",
      "        |        0 |        0 |\n",
      "And             | and             | CCONJ    | CC       | cc          | Xxx      |        1 |        1 |\n",
      "yet             | yet             | ADV      | RB       | advmod      | xxx      |        1 |        1 |\n",
      ",               | ,               | PUNCT    | ,        | punct       | ,        |        0 |        0 |\n",
      "you             | you             | PRON     | PRP      | nsubj       | xxx      |        1 |        1 |\n",
      "really          | really          | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
      "believe         | believe         | VERB     | VBP      | ROOT        | xxxx     |        1 |        0 |\n",
      "that            | that            | SCONJ    | IN       | mark        | xxxx     |        1 |        1 |\n",
      "this            | this            | PRON     | DT       | nsubj       | xxxx     |        1 |        1 |\n",
      "is              | be              | AUX      | VBZ      | ccomp       | xx       |        1 |        1 |\n",
      "a               | a               | DET      | DT       | det         | x        |        1 |        1 |\n",
      "hospital        | hospital        | NOUN     | NN       | attr        | xxxx     |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "\n",
      "               | \n",
      "               | SPACE    | _SP      | dep         | \n",
      "        |        0 |        0 |\n",
      "The             | the             | DET      | DT       | det         | Xxx      |        1 |        1 |\n",
      "producers       | producer        | NOUN     | NNS      | nsubj       | xxxx     |        1 |        0 |\n",
      "were            | be              | AUX      | VBD      | ROOT        | xxxx     |        1 |        1 |\n",
      "very            | very            | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
      "careful         | careful         | ADJ      | JJ       | acomp       | xxxx     |        1 |        0 |\n",
      "to              | to              | PART     | TO       | aux         | xx       |        1 |        1 |\n",
      "include         | include         | VERB     | VB       | xcomp       | xxxx     |        1 |        0 |\n",
      "real            | real            | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
      "medical         | medical         | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
      "terminology     | terminology     | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
      "and             | and             | CCONJ    | CC       | cc          | xxx      |        1 |        1 |\n",
      "real            | real            | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
      "medical         | medical         | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
      "cases           | case            | NOUN     | NNS      | conj        | xxxx     |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "\n",
      "               | \n",
      "               | SPACE    | _SP      | dep         | \n",
      "        |        0 |        0 |\n",
      "This            | this            | DET      | DT       | det         | Xxxx     |        1 |        1 |\n",
      "movie           | movie           | NOUN     | NN       | nsubj       | xxxx     |        1 |        0 |\n",
      "really          | really          | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
      "reveals         | reveal          | VERB     | VBZ      | ROOT        | xxxx     |        1 |        0 |\n",
      "how             | how             | SCONJ    | WRB      | advmod      | xxx      |        1 |        1 |\n",
      "difficult       | difficult       | ADJ      | JJ       | ccomp       | xxxx     |        1 |        0 |\n",
      "in              | in              | SCONJ    | IN       | mark        | xx       |        1 |        1 |\n",
      "is              | be              | AUX      | VBZ      | advcl       | xx       |        1 |        1 |\n",
      "to              | to              | PART     | TO       | aux         | xx       |        1 |        1 |\n",
      "run             | run             | VERB     | VB       | xcomp       | xxx      |        1 |        0 |\n",
      "a               | a               | DET      | DT       | det         | x        |        1 |        1 |\n",
      "hospital        | hospital        | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
      ",               | ,               | PUNCT    | ,        | punct       | ,        |        0 |        0 |\n",
      "and             | and             | CCONJ    | CC       | cc          | xxx      |        1 |        1 |\n",
      "how             | how             | SCONJ    | WRB      | advmod      | xxx      |        1 |        1 |\n",
      "badly           | badly           | ADV      | RB       | advmod      | xxxx     |        1 |        0 |\n",
      "things          | thing           | NOUN     | NNS      | nsubj       | xxxx     |        1 |        0 |\n",
      "already         | already         | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
      "were            | be              | AUX      | VBD      | conj        | xxxx     |        1 |        1 |\n",
      "in              | in              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
      "1971            | 1971            | NUM      | CD       | pobj        | dddd     |        0 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "\n",
      "               | \n",
      "               | SPACE    | _SP      | dep         | \n",
      "        |        0 |        0 |\n",
      "I               | I               | PRON     | PRP      | nsubj       | X        |        1 |        1 |\n",
      "loved           | love            | VERB     | VBD      | ROOT        | xxxx     |        1 |        0 |\n",
      "this            | this            | DET      | DT       | det         | xxxx     |        1 |        1 |\n",
      "movie           | movie           | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "P.S.            | P.S.            | PROPN    | NNP      | npadvmod    | X.X.     |        0 |        0 |\n",
      "-               | -               | PUNCT    | :        | punct       | -        |        0 |        0 |\n",
      "I               | I               | PRON     | PRP      | nsubj       | X        |        1 |        1 |\n",
      "noticed         | notice          | VERB     | VBD      | ROOT        | xxxx     |        1 |        0 |\n",
      "that            | that            | SCONJ    | IN       | mark        | xxxx     |        1 |        1 |\n",
      "the             | the             | DET      | DT       | det         | xxx      |        1 |        1 |\n",
      "incompetent     | incompetent     | ADJ      | JJ       | nsubj       | xxxx     |        1 |        0 |\n",
      ",               | ,               | PUNCT    | ,        | punct       | ,        |        0 |        0 |\n",
      "wheeler         | wheeler         | NOUN     | NN       | compound    | xxxx     |        1 |        0 |\n",
      "dealer          | dealer          | NOUN     | NN       | compound    | xxxx     |        1 |        0 |\n",
      "surgeon         | surgeon         | NOUN     | NN       | nsubj       | xxxx     |        1 |        0 |\n",
      "played          | play            | VERB     | VBD      | ccomp       | xxxx     |        1 |        0 |\n",
      "the             | the             | DET      | DT       | det         | xxx      |        1 |        1 |\n",
      "head            | head            | NOUN     | NN       | dobj        | xxxx     |        1 |        0 |\n",
      "of              | of              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
      "the             | the             | DET      | DT       | det         | xxx      |        1 |        1 |\n",
      "firm            | firm            | NOUN     | NN       | pobj        | xxxx     |        1 |        0 |\n",
      "in              | in              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
      "\n",
      "               | \n",
      "               | SPACE    | _SP      | dep         | \n",
      "        |        0 |        0 |\n",
      "LA              | LA              | PROPN    | NNP      | compound    | XX       |        1 |        0 |\n",
      "Law             | Law             | PROPN    | NNP      | pobj        | Xxx      |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "The             | the             | DET      | DT       | det         | Xxx      |        1 |        1 |\n",
      "young           | young           | ADJ      | JJ       | amod        | xxxx     |        1 |        0 |\n",
      "doctor          | doctor          | NOUN     | NN       | nsubj       | xxxx     |        1 |        0 |\n",
      "played          | play            | VERB     | VBD      | ROOT        | xxxx     |        1 |        0 |\n",
      "in              | in              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
      "Lou             | Lou             | PROPN    | NNP      | compound    | Xxx      |        1 |        0 |\n",
      "Grant           | Grant           | PROPN    | NNP      | pobj        | Xxxxx    |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "\n",
      "               | \n",
      "               | SPACE    | _SP      | dep         | \n",
      "        |        0 |        0 |\n",
      "I               | I               | PRON     | PRP      | nsubj       | X        |        1 |        1 |\n",
      "also            | also            | ADV      | RB       | advmod      | xxxx     |        1 |        1 |\n",
      "noticed         | notice          | VERB     | VBD      | ROOT        | xxxx     |        1 |        0 |\n",
      "that            | that            | SCONJ    | IN       | mark        | xxxx     |        1 |        1 |\n",
      "the             | the             | DET      | DT       | det         | xxx      |        1 |        1 |\n",
      "registration    | registration    | NOUN     | NN       | compound    | xxxx     |        1 |        0 |\n",
      "nurse           | nurse           | NOUN     | NN       | nsubj       | xxxx     |        1 |        0 |\n",
      "has             | have            | AUX      | VBZ      | aux         | xxx      |        1 |        1 |\n",
      "appeared        | appear          | VERB     | VBN      | ccomp       | xxxx     |        1 |        0 |\n",
      "since           | since           | ADV      | RB       | prep        | xxxx     |        1 |        1 |\n",
      "in              | in              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
      "Becker          | Becker          | PROPN    | NNP      | pobj        | Xxxxx    |        1 |        0 |\n",
      "and             | and             | CCONJ    | CC       | cc          | xxx      |        1 |        1 |\n",
      "other           | other           | ADJ      | JJ       | amod        | xxxx     |        1 |        1 |\n",
      "shows           | show            | NOUN     | NNS      | conj        | xxxx     |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "\n",
      "               | \n",
      "               | SPACE    | _SP      | dep         | \n",
      "        |        0 |        0 |\n"
     ]
    }
   ],
   "source": [
    "print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} | '.format('TEXT','LEMMA_','POS_','TAG_','DEP_','SHAPE_','IS_ALPHA','IS_STOP'))\n",
    "\n",
    "for token in doc:\n",
    "    print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} |'.format(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For any help\n",
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greatest_films\n",
      "brilliant_satire\n",
      "medical_profession\n",
      "pretty_picture\n",
      "Healthy_patients\n",
      "incompetent_surgeons\n",
      "medical_terminology\n",
      "medical_cases\n",
      "young_doctor\n",
      "other_shows\n"
     ]
    }
   ],
   "source": [
    "previous_token = doc[0]  # set first token\n",
    "\n",
    "for token in doc[1:]:    \n",
    "    # identify adjective noun pairs\n",
    "    if previous_token.pos_ == 'ADJ' and token.pos_ == 'NOUN':\n",
    "        print(f'{previous_token.text}_{token.text}')\n",
    "    \n",
    "    previous_token = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL      | ENTITY         \n",
      "GPE        | India                                             \n"
     ]
    }
   ],
   "source": [
    "ner_text = \"I love my country India, and I love Machine learning\"\n",
    "\n",
    "ner_doc = nlp(ner_text)\n",
    "print('{:10} | {:15}'.format('LABEL','ENTITY'))\n",
    "\n",
    "for ent in ner_doc.ents:\n",
    "    print('{:10} | {:50}'.format(ent.label_, ent.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I love my country \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", and I love Machine learning</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(docs=ner_doc, style='ent', jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
